{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fafbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "#import fitz \n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6086f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== SETTINGS ======\n",
    "EXCEL_FILE = r\"E:\\Income Tax Fine-tuning\\Web Scraping URL's.xlsx\"\n",
    "URL_COLUMN = \"Source\"\n",
    "SAVE_HTML_DIR = \"scraped_html\"\n",
    "SAVE_PDF_DIR = \"scraped_pdfs\"\n",
    "COMBINED_OUTPUT = \"indian_tax_code_combined.txt\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4cba655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== SETUP ======\n",
    "os.makedirs(SAVE_HTML_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_PDF_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65a9d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== UTILITY FUNCTIONS ======\n",
    "\n",
    "def normalize_url(url):\n",
    "    url = str(url).strip()\n",
    "    if url.lower().startswith(\"wikipedia\"):\n",
    "        return \"https://en.wikipedia.org/wiki/Income_tax_in_India\"\n",
    "    if url and not url.startswith(\"http\"):\n",
    "        if \".\" in url:\n",
    "            return \"https://\" + url\n",
    "    return url if url.startswith(\"http\") else \"\"\n",
    "\n",
    "def load_urls_from_excel(file_path, url_column=\"URL\"):\n",
    "    df = pd.read_excel(file_path)\n",
    "    urls = (\n",
    "        df[url_column]\n",
    "        .astype(str)\n",
    "        .apply(lambda x: x.strip())\n",
    "    )\n",
    "    # Only keep rows where the URL string length is reasonable\n",
    "    urls = urls[urls.str.len() > 5]\n",
    "    urls = urls.apply(normalize_url)\n",
    "    urls = [u for u in urls if u.startswith(\"http\")]\n",
    "    print(f\"[INFO] Loaded {len(urls)} valid URLs after cleaning\")\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        res.raise_for_status()\n",
    "        return res.text\n",
    "    except Exception as e:\n",
    "        print(f\"[HTML ERROR] {url} -> {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_html_to_text(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
    "        tag.decompose()\n",
    "    text = html2text.html2text(soup.get_text())\n",
    "    return '\\n'.join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "\n",
    "def extract_text_from_pdf(url, save_path):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS)\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        text = \"\"\n",
    "        with fitz.open(save_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[PDF ERROR] {url} -> {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c237c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== SCRAPER MAIN ======\n",
    "\n",
    "def run_scraper_from_excel(excel_file):\n",
    "    urls = load_urls_from_excel(excel_file, url_column=URL_COLUMN)\n",
    "    print(f\"[INFO] Loaded {len(urls)} valid URLs from {excel_file}\")\n",
    "\n",
    "    all_texts = []\n",
    "    failed_urls = []\n",
    "\n",
    "    for i, url in enumerate(tqdm(urls, desc=\"Scraping\")):\n",
    "        if not url.startswith(\"http\"):\n",
    "            print(f\"[SKIPPED] Invalid URL format: {url}\")\n",
    "            failed_urls.append((url, \"Invalid format\"))\n",
    "            continue\n",
    "\n",
    "        filename_prefix = f\"doc_{i:03}\"\n",
    "        text = \"\"\n",
    "\n",
    "        try:\n",
    "            if url.lower().endswith(\".pdf\"):\n",
    "                save_path = os.path.join(SAVE_PDF_DIR, filename_prefix + \".pdf\")\n",
    "                text = extract_text_from_pdf(url, save_path)\n",
    "            else:\n",
    "                html = get_html(url)\n",
    "                if not html:\n",
    "                    failed_urls.append((url, \"Empty HTML response\"))\n",
    "                    continue\n",
    "                text = clean_html_to_text(html)\n",
    "                save_path = os.path.join(SAVE_HTML_DIR, filename_prefix + \".txt\")\n",
    "                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(text)\n",
    "\n",
    "            # Save with metadata\n",
    "            if len(text.strip()) > 100:\n",
    "                all_texts.append(f\"[SOURCE: {url}]\\n{text.strip()}\")\n",
    "            else:\n",
    "                failed_urls.append((url, \"Too little text\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {url} -> {e}\")\n",
    "            failed_urls.append((url, str(e)))\n",
    "\n",
    "        # Smart sleep\n",
    "        if any(d in url for d in [\"cleartax\", \"taxmann\"]):\n",
    "            time.sleep(2.5)\n",
    "        else:\n",
    "            time.sleep(1.0)\n",
    "\n",
    "    # Combine all into one file\n",
    "    with open(COMBINED_OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n\".join(all_texts))\n",
    "\n",
    "    print(f\"[✅ DONE] Combined output saved to: {COMBINED_OUTPUT}\")\n",
    "\n",
    "    # Save failed URL log\n",
    "    if failed_urls:\n",
    "        log_file = \"scraping_failed_urls.csv\"\n",
    "        pd.DataFrame(failed_urls, columns=[\"URL\", \"Reason\"]).to_csv(log_file, index=False)\n",
    "        print(f\"[⚠️ LOG] Failed or skipped URLs logged in: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2726314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 11 valid URLs after cleaning\n",
      "[INFO] Loaded 11 valid URLs from E:\\Income Tax Fine-tuning\\Web Scraping URL's.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping:   9%|▉         | 1/11 [00:00<00:02,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HTML ERROR] https://incometaxindia.gov.in -> 503 Server Error: Service Temporarily Unavailable for url: https://incometaxindia.gov.in/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping:  27%|██▋       | 3/11 [00:03<00:09,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HTML ERROR] https://egazette.nic.in -> HTTPSConnectionPool(host='egazette.nic.in', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001919844F8E0>: Failed to resolve 'egazette.nic.in' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping:  91%|█████████ | 10/11 [00:14<00:01,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HTML ERROR] https://vakilno1.com -> 410 Client Error: Gone for url: https://vakilno1.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping: 100%|██████████| 11/11 [00:15<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HTML ERROR] https://legalserviceindia.com -> 406 Client Error: Not Acceptable for url: https://legalserviceindia.com/\n",
      "[✅ DONE] Combined output saved to: indian_tax_code_combined.txt\n",
      "[⚠️ LOG] Failed or skipped URLs logged in: scraping_failed_urls.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== RUN ======\n",
    "run_scraper_from_excel(EXCEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2436bf",
   "metadata": {},
   "source": [
    "# Special case URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae2ffde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "import socket\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from urllib.parse import urlparse\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0559b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ========== NETWORK CONNECTIVITY CHECK ==========\n",
    "def check_internet_connection():\n",
    "    \"\"\"Check if internet connection is available\"\"\"\n",
    "    try:\n",
    "        socket.create_connection((\"8.8.8.8\", 53), timeout=5)\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def check_domain_resolution(domain):\n",
    "    \"\"\"Check if a specific domain can be resolved\"\"\"\n",
    "    try:\n",
    "        socket.gethostbyname(domain)\n",
    "        return True\n",
    "    except socket.gaierror:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4cf569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== PDF TEXT EXTRACTION ==========\n",
    "def extract_text(pdf_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if not os.path.exists(pdf_path):\n",
    "            logger.error(f\"PDF file not found: {pdf_path}\")\n",
    "            return \"\"\n",
    "        \n",
    "        reader = PdfReader(pdf_path)\n",
    "        for page_num, page in enumerate(reader.pages):\n",
    "            try:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error extracting text from page {page_num}: {e}\")\n",
    "                continue\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"PDF extraction error for {pdf_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2186027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ROBUST HTTP SESSION ==========\n",
    "def create_robust_session():\n",
    "    \"\"\"Create a robust HTTP session with retries and timeouts\"\"\"\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Configure retries\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "        backoff_factor=2\n",
    "    )\n",
    "    \n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    # Set headers\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    })\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feebb1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SELENIUM SETUP ==========\n",
    "def create_selenium_driver():\n",
    "    \"\"\"Create a robust Selenium WebDriver\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to create Chrome driver: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0b0765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SCRAPE incometaxindia.gov.in ==========\n",
    "def scrape_incometaxindia_with_selenium():\n",
    "    \"\"\"Scrape Income Tax India website using Selenium\"\"\"\n",
    "    logger.info(\"Starting scrape of incometaxindia.gov.in\")\n",
    "    \n",
    "    # Check domain resolution first\n",
    "    if not check_domain_resolution(\"incometaxindia.gov.in\"):\n",
    "        logger.error(\"Cannot resolve incometaxindia.gov.in\")\n",
    "        return \"\"\n",
    "    \n",
    "    driver = create_selenium_driver()\n",
    "    if not driver:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        url = \"https://incometaxindia.gov.in\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "        \n",
    "        # Clean up text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        text = '\\n'.join(line for line in lines if line)\n",
    "        \n",
    "        with open(\"scraped_incometaxindia.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        logger.info(\"Successfully scraped incometaxindia.gov.in\")\n",
    "        return text\n",
    "        \n",
    "    except TimeoutException:\n",
    "        logger.error(\"Timeout while loading incometaxindia.gov.in\")\n",
    "        return \"\"\n",
    "    except WebDriverException as e:\n",
    "        logger.error(f\"WebDriver error for incometaxindia.gov.in: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error scraping incometaxindia.gov.in: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "# ========== SCRAPE egazette.nic.in (PDF Example) ==========\n",
    "def download_sample_egazette_pdf():\n",
    "    \"\"\"Download and extract text from egazette PDF\"\"\"\n",
    "    logger.info(\"Starting egazette.nic.in PDF download\")\n",
    "    \n",
    "    # Check domain resolution first\n",
    "    if not check_domain_resolution(\"egazette.nic.in\"):\n",
    "        logger.error(\"Cannot resolve egazette.nic.in\")\n",
    "        return \"\"\n",
    "    \n",
    "    session = create_robust_session()\n",
    "    pdf_url = \"https://egazette.nic.in/WriteReadData/2023/248929.pdf\"\n",
    "    \n",
    "    try:\n",
    "        response = session.get(pdf_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        pdf_filename = \"egazette_sample.pdf\"\n",
    "        with open(pdf_filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        logger.info(f\"Downloaded PDF: {pdf_filename}\")\n",
    "        \n",
    "        text = extract_text(pdf_filename)\n",
    "        if text:\n",
    "            with open(\"scraped_egazette.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "            logger.info(\"Successfully extracted text from egazette PDF\")\n",
    "        else:\n",
    "            logger.warning(\"No text extracted from egazette PDF\")\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Network error downloading egazette PDF: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error with egazette PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========== SCRAPE taxmann.com ==========\n",
    "def scrape_taxmann_sample():\n",
    "    \"\"\"Scrape Taxmann website\"\"\"\n",
    "    logger.info(\"Starting scrape of taxmann.com\")\n",
    "    \n",
    "    # Check domain resolution first\n",
    "    if not check_domain_resolution(\"taxmann.com\"):\n",
    "        logger.error(\"Cannot resolve taxmann.com\")\n",
    "        return \"\"\n",
    "    \n",
    "    driver = create_selenium_driver()\n",
    "    if not driver:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Try to access a more general page first\n",
    "        url = \"https://www.taxmann.com\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Try to find main content\n",
    "        content = (soup.find(\"main\") or \n",
    "                  soup.find(\"div\", class_=\"content\") or \n",
    "                  soup.find(\"div\", id=\"content\") or\n",
    "                  soup.find(\"body\"))\n",
    "        \n",
    "        text = content.get_text(separator=\"\\n\") if content else \"\"\n",
    "        \n",
    "        # Clean up text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        text = '\\n'.join(line for line in lines if line)\n",
    "        \n",
    "        with open(\"scraped_taxmann.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        logger.info(\"Successfully scraped taxmann.com\")\n",
    "        return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error scraping taxmann.com: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de72bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ROBUST SCRAPING WITH REQUESTS ==========\n",
    "def robust_scrape_with_requests(url, output_file):\n",
    "    \"\"\"Robust scraping using requests with comprehensive error handling\"\"\"\n",
    "    logger.info(f\"Starting scrape of {url}\")\n",
    "    \n",
    "    # Parse URL to get domain\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    \n",
    "    # Check domain resolution\n",
    "    if not check_domain_resolution(domain):\n",
    "        logger.error(f\"Cannot resolve domain: {domain}\")\n",
    "        return \"\"\n",
    "    \n",
    "    session = create_robust_session()\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "        \n",
    "        # Clean up text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        text = '\\n'.join(line for line in lines if line)\n",
    "        \n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        \n",
    "        logger.info(f\"Successfully scraped {url}\")\n",
    "        return text\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Network error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error scraping {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51bf5d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 18:03:47,193 - INFO - Starting comprehensive scraping process\n",
      "2025-06-18 18:03:47,294 - INFO - Processing incometaxindia.gov.in...\n",
      "2025-06-18 18:03:47,295 - INFO - Starting scrape of incometaxindia.gov.in\n",
      "2025-06-18 18:03:53,350 - INFO - Successfully scraped incometaxindia.gov.in\n",
      "2025-06-18 18:03:59,528 - INFO - ✅ Successfully processed incometaxindia.gov.in\n",
      "2025-06-18 18:04:01,539 - INFO - Processing egazette.nic.in...\n",
      "2025-06-18 18:04:01,539 - INFO - Starting egazette.nic.in PDF download\n",
      "2025-06-18 18:04:01,542 - ERROR - Cannot resolve egazette.nic.in\n",
      "2025-06-18 18:04:01,548 - WARNING - ⚠️  No content retrieved from egazette.nic.in\n",
      "2025-06-18 18:04:03,562 - INFO - Processing taxmann.com...\n",
      "2025-06-18 18:04:03,562 - INFO - Starting scrape of taxmann.com\n",
      "2025-06-18 18:04:15,952 - INFO - Successfully scraped taxmann.com\n",
      "2025-06-18 18:04:22,069 - INFO - ✅ Successfully processed taxmann.com\n",
      "2025-06-18 18:04:24,078 - INFO - Processing vakilno1.com...\n",
      "2025-06-18 18:04:24,078 - INFO - Starting scrape of https://vakilno1.com\n",
      "2025-06-18 18:04:24,129 - ERROR - ❌ Failed to process vakilno1.com: __init__() got an unexpected keyword argument 'method_whitelist'\n",
      "2025-06-18 18:04:26,139 - INFO - Processing legalserviceindia.com...\n",
      "2025-06-18 18:04:26,139 - INFO - Starting scrape of https://legalserviceindia.com\n",
      "2025-06-18 18:04:26,495 - ERROR - ❌ Failed to process legalserviceindia.com: __init__() got an unexpected keyword argument 'method_whitelist'\n",
      "2025-06-18 18:04:28,511 - INFO - ✅ COMPLETED: Combined all successful scrapes into: indian_tax_code_special_sources.txt\n",
      "2025-06-18 18:04:28,511 - INFO - Successfully processed 2 out of 5 sources\n"
     ]
    }
   ],
   "source": [
    "# ========== MAIN EXECUTION FUNCTION ==========\n",
    "def run_all_special_scrapers():\n",
    "    \"\"\"Run all scrapers with comprehensive error handling\"\"\"\n",
    "    logger.info(\"Starting comprehensive scraping process\")\n",
    "    \n",
    "    # Check internet connectivity first\n",
    "    if not check_internet_connection():\n",
    "        logger.error(\"No internet connection detected. Please check your network.\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"special_scrape_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.chdir(output_dir)\n",
    "    \n",
    "    combined_texts = []\n",
    "    scrapers = [\n",
    "        (\"incometaxindia.gov.in\", scrape_incometaxindia_with_selenium),\n",
    "        (\"egazette.nic.in\", download_sample_egazette_pdf),\n",
    "        (\"taxmann.com\", scrape_taxmann_sample),\n",
    "        (\"vakilno1.com\", lambda: robust_scrape_with_requests(\"https://vakilno1.com\", \"scraped_vakilno1.txt\")),\n",
    "        (\"legalserviceindia.com\", lambda: robust_scrape_with_requests(\"https://legalserviceindia.com\", \"scraped_legalserviceindia.txt\"))\n",
    "    ]\n",
    "    \n",
    "    for source_name, scraper_func in scrapers:\n",
    "        try:\n",
    "            logger.info(f\"Processing {source_name}...\")\n",
    "            text = scraper_func()\n",
    "            \n",
    "            if text.strip():\n",
    "                combined_texts.append(f\"[SOURCE: {source_name}]\\n{text}\")\n",
    "                logger.info(f\"✅ Successfully processed {source_name}\")\n",
    "            else:\n",
    "                logger.warning(f\"⚠️  No content retrieved from {source_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to process {source_name}: {e}\")\n",
    "        \n",
    "        # Wait between requests to be respectful\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Combine all successful outputs\n",
    "    if combined_texts:\n",
    "        combined_content = \"\\n\\n\" + \"=\"*80 + \"\\n\\n\".join(combined_texts)\n",
    "        \n",
    "        with open(\"indian_tax_code_special_sources.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(combined_content)\n",
    "        \n",
    "        logger.info(\"✅ COMPLETED: Combined all successful scrapes into: indian_tax_code_special_sources.txt\")\n",
    "        logger.info(f\"Successfully processed {len(combined_texts)} out of {len(scrapers)} sources\")\n",
    "    else:\n",
    "        logger.warning(\"⚠️  No content was successfully scraped from any source\")\n",
    "\n",
    "# ========== EXECUTE ==========\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_special_scrapers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d718b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅ DONE] Combined file saved to: indian_tax_code_final_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "def combine_text_files(file1, file2, output_file):\n",
    "    with open(file1, \"r\", encoding=\"utf-8\") as f1, \\\n",
    "         open(file2, \"r\", encoding=\"utf-8\") as f2, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        \n",
    "        content1 = f1.read().strip()\n",
    "        content2 = f2.read().strip()\n",
    "        \n",
    "        combined = content1 + \"\\n\\n\" + content2\n",
    "        out.write(combined)\n",
    "    \n",
    "    print(f\"[✅ DONE] Combined file saved to: {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "combine_text_files(\n",
    "    r\"E:\\Income Tax Fine-tuning\\indian_tax_code_combined.txt\",\n",
    "    r\"E:\\Income Tax Fine-tuning\\indian_tax_code_special_sources.txt\",\n",
    "    \"indian_tax_code_final_corpus.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
